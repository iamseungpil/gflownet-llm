"""
Complete Multi-Model ARC Experiment
Î™®Îì† Î™®Îç∏Î°ú RE-ARC, H-ARC(Í∑∏Î¶¨Îìú), H-ARC(Ïï°ÏÖò) Ïã§ÌóòÏùÑ ÌÜµÌï© Ïã§Ìñâ
"""

import json
import os
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Union
import re
import wandb
from tqdm import tqdm
from arc import train_problems, validation_problems
from collections import defaultdict
from difflib import SequenceMatcher
from openai import OpenAI
from abc import ABC, abstractmethod

# Color mapping for ARC grids
COLOR_MAP = {
    0: "Black", 1: "Blue", 2: "Red", 3: "Green", 4: "Yellow",
    5: "Gray", 6: "Pink", 7: "Orange", 8: "Purple", 9: "Brown"
}

REVERSE_COLOR_MAP = {v: k for k, v in COLOR_MAP.items()}


class BaseARCModel(ABC):
    """Î™®Îì† Î™®Îç∏Ïóê ÎåÄÌïú Í∏∞Î≥∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§"""
    
    @abstractmethod
    def generate_response(self, prompt: str, max_tokens: int = 512, temperature: float = 0.1) -> str:
        pass
    
    @abstractmethod
    def get_model_name(self) -> str:
        pass


class HuggingFaceModel(BaseARCModel):
    """Hugging Face Î™®Îç∏Ïö© ÎûòÌçº"""
    
    def __init__(self, model_name: str, device: str = None):
        self.model_name = model_name
        self.device = torch.device(device if device else ("cuda" if torch.cuda.is_available() else "cpu"))
        
        print(f"Loading {model_name}...")
        cache_dir = os.getenv('HF_HOME', '/data/.cache/huggingface')
        print(f"Using cache directory: {cache_dir}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
        
        if "qwen" in model_name.lower():
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            cache_dir=cache_dir
        )
        self.model.eval()
    
    def generate_response(self, prompt: str, max_tokens: int = 512, temperature: float = 0.1) -> str:
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, 
                               max_length=2048).to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], 
                                       skip_special_tokens=True)
        return response
    
    def get_model_name(self) -> str:
        return self.model_name


class OpenAIModel(BaseARCModel):
    """OpenAI Î™®Îç∏Ïö© ÎûòÌçº"""
    
    def __init__(self, model_name: str = "o1-preview", api_key: str = None):
        self.model_name = model_name
        
        if api_key:
            self.client = OpenAI(api_key=api_key)
        else:
            self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        print(f"Initialized OpenAI {model_name}")
    
    def generate_response(self, prompt: str, max_tokens: int = 512, temperature: float = 0.1) -> str:
        try:
            if "o1" in self.model_name:
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[{"role": "user", "content": prompt}],
                    max_completion_tokens=max_tokens
                )
            else:
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    temperature=temperature
                )
            
            return response.choices[0].message.content
        except Exception as e:
            print(f"OpenAI API error: {e}")
            return ""
    
    def get_model_name(self) -> str:
        return f"openai/{self.model_name}"


class CompleteMultiModelExperiment:
    """Î™®Îì† Î™®Îç∏Î°ú Î™®Îì† Ïã§ÌóòÏùÑ ÏàòÌñâÌïòÎäî ÌÜµÌï© ÌÅ¥ÎûòÏä§"""
    
    def __init__(self, models_config: Dict[str, Dict], use_wandb: bool = True):
        self.models = {}
        self.models_config = models_config
        
        # Î™®Îç∏ Ï¥àÍ∏∞Ìôî
        for model_id, config in models_config.items():
            if config["type"] == "huggingface":
                self.models[model_id] = HuggingFaceModel(config["model_name"])
            elif config["type"] == "openai":
                self.models[model_id] = OpenAIModel(
                    config["model_name"], 
                    config.get("api_key")
                )
        
        # Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú ÏÑ§Ï†ï
        self.rearc_path = Path("data/re-arc/re_arc_extracted/re_arc/tasks")
        self.harc_data_path = Path("data/h-arc/data/data.csv")
        
        # Action Îß§Ìïë Ï†ïÏùò
        self.action_mapping = {
            'click_cell': ['click', 'select_cell', 'cell_click'],
            'select_color': ['color', 'pick_color', 'choose_color'],
            'flood_fill': ['fill', 'bucket_fill', 'paint'],
            'copy': ['copy', 'copy_selection'],
            'paste': ['paste', 'paste_selection'],
            'resize_grid': ['resize', 'resize_output', 'change_size'],
            'rotate': ['rotate', 'rotation', 'turn'],
            'flip': ['flip', 'mirror'],
            'submit': ['submit', 'done', 'finish']
        }
        
        # W&B Ï¥àÍ∏∞Ìôî
        self.use_wandb = use_wandb
        if self.use_wandb:
            wandb.login(key="2f4e627868f1f9dad10bcb1a14fbf96817e6baa9")
            wandb.init(
                project="arc-complete-multi-model",
                config={
                    "models": list(models_config.keys()),
                    "experiment_type": "complete_multi_model"
                }
            )
    
    def load_arc_task(self, task_id: str):
        """ÏõêÎ≥∏ ARC ÌÉúÏä§ÌÅ¨ Î°úÎìú"""
        all_problems = train_problems + validation_problems
        
        for problem in all_problems:
            if problem.uid == task_id:
                return problem
        return None
    
    def load_rearc_augmentations(self, task_id: str) -> List[Dict]:
        """RE-ARC Ï¶ùÍ∞ï Îç∞Ïù¥ÌÑ∞ Î°úÎìú"""
        augmented_examples = []
        rearc_file = self.rearc_path / f"{task_id}.json"
        
        if rearc_file.exists():
            with open(rearc_file, 'r') as f:
                rearc_data = json.load(f)
                
                if isinstance(rearc_data, list):
                    augmented_examples = rearc_data
                elif isinstance(rearc_data, dict):
                    if 'train' in rearc_data:
                        augmented_examples = rearc_data.get('train', [])
                
                valid_examples = []
                for example in augmented_examples:
                    if isinstance(example, dict) and 'input' in example and 'output' in example:
                        valid_examples.append(example)
                
                augmented_examples = valid_examples[:20]  # ÏµúÎåÄ 20Í∞úÎßå ÏÇ¨Ïö©
        
        return augmented_examples
    
    def load_harc_action_traces(self, task_id: str) -> List[List[Dict]]:
        """H-ARC action trace Î°úÎìú"""
        if not self.harc_data_path.exists():
            print(f"H-ARC data file not found: {self.harc_data_path}")
            return []
        
        df = pd.read_csv(self.harc_data_path)
        task_data = df[df['task_name'] == task_id]
        
        if len(task_data) == 0:
            return []
        
        successful_attempts = task_data[task_data['solved'] == True]
        action_traces = []
        unique_attempts = successful_attempts[['hashed_id', 'attempt_number']].drop_duplicates()
        
        for _, row in unique_attempts.iterrows():
            participant_id = row['hashed_id']
            attempt_num = row['attempt_number']
            
            attempt_data = task_data[
                (task_data['hashed_id'] == participant_id) & 
                (task_data['attempt_number'] == attempt_num)
            ].sort_values('action_id')
            
            actions = []
            for _, action_row in attempt_data.iterrows():
                action_info = {
                    'action': action_row['action'],
                    'action_id': action_row['action_id'],
                    'selected_tool': action_row.get('selected_tool', None),
                    'selected_symbol': action_row.get('selected_symbol', None),
                    'solved': action_row['solved']
                }
                actions.append(action_info)
            
            if actions:
                action_traces.append(actions)
        
        return action_traces
    
    def grid_to_string(self, grid: List[List[int]], use_colors: bool = False) -> str:
        """Í∑∏Î¶¨ÎìúÎ•º Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôò"""
        if use_colors:
            rows = []
            for row in grid:
                color_row = [COLOR_MAP.get(int(cell), f"Unknown_{cell}") for cell in row]
                rows.append(" ".join(color_row))
            return "\n".join(rows)
        else:
            return '\n'.join([' '.join(map(str, row)) for row in grid])
    
    def string_to_grid(self, grid_str: str, from_colors: bool = False) -> List[List[int]]:
        """Î¨∏ÏûêÏó¥ÏùÑ Í∑∏Î¶¨ÎìúÎ°ú Î≥ÄÌôò"""
        try:
            lines = grid_str.strip().split('\n')
            grid = []
            for line in lines:
                if line.strip():
                    if from_colors:
                        colors = line.split()
                        row = [REVERSE_COLOR_MAP.get(color, -1) for color in colors]
                    else:
                        row = [int(x) for x in line.split()]
                    grid.append(row)
            return grid
        except:
            return []
    
    def extract_grid_from_response(self, response: str, from_colors: bool = False) -> List[List[int]]:
        """ÏùëÎãµÏóêÏÑú Í∑∏Î¶¨Îìú Ï∂îÏ∂ú"""
        lines = response.strip().split('\n')
        grid_lines = []
        
        if from_colors:
            color_pattern = '|'.join(COLOR_MAP.values())
            for line in lines:
                if re.search(color_pattern, line, re.IGNORECASE):
                    grid_lines.append(line.strip())
        else:
            for line in lines:
                if re.match(r'^[\d\s]+$', line.strip()) and line.strip():
                    grid_lines.append(line.strip())
        
        if grid_lines:
            return self.string_to_grid('\n'.join(grid_lines), from_colors)
        return []
    
    def evaluate_solution(self, predicted: List[List[int]], expected: List[List[int]]) -> bool:
        """ÏòàÏ∏°Í≥º Ï†ïÎãµ ÎπÑÍµê"""
        if not predicted or not expected:
            return False
        
        if len(predicted) != len(expected):
            return False
        
        for pred_row, exp_row in zip(predicted, expected):
            if len(pred_row) != len(exp_row):
                return False
            if pred_row != exp_row:
                return False
        
        return True
    
    # ============= RE-ARC Ïã§Ìóò Î©îÏÑúÎìú =============
    
    def create_baseline_prompt(self, task: Dict, use_colors: bool = False) -> str:
        """Í∏∞Î≥∏ ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±"""
        if use_colors:
            prompt = """You are solving an ARC task. Find the pattern and apply it to the test input.
Colors: Black(0), Blue(1), Red(2), Green(3), Yellow(4), Gray(5), Pink(6), Orange(7), Purple(8), Brown(9).

Training Examples:
"""
        else:
            prompt = """You are solving an ARC task. Find the pattern and apply it to the test input.

Training Examples:
"""
        
        for i, pair in enumerate(task.train_pairs):
            prompt += f"\nExample {i+1}:\nInput:\n{self.grid_to_string(pair.x.tolist(), use_colors)}\n"
            prompt += f"Output:\n{self.grid_to_string(pair.y.tolist(), use_colors)}\n"
        
        prompt += f"\nTest Input:\n{self.grid_to_string(task.test_pairs[0].x.tolist(), use_colors)}\n"
        prompt += "\nProvide only the output grid in the same format:"
        
        return prompt
    
    def create_augmented_prompt(self, task: Dict, augmented_examples: List[Dict], 
                               num_augmented: int = 5, use_colors: bool = False) -> str:
        """Ï¶ùÍ∞ï Îç∞Ïù¥ÌÑ∞ Ìè¨Ìï® ÌîÑÎ°¨ÌîÑÌä∏"""
        prompt = self.create_baseline_prompt(task, use_colors)
        
        if augmented_examples and num_augmented > 0:
            selected_examples = augmented_examples[:num_augmented]
            prompt = prompt.replace("Training Examples:", "Original Training Examples:")
            
            additional_prompt = f"\nAdditional Examples:"
            for i, example in enumerate(selected_examples):
                additional_prompt += f"\nAugmented Example {i+1}:\nInput:\n{self.grid_to_string(example['input'], use_colors)}\n"
                additional_prompt += f"Output:\n{self.grid_to_string(example['output'], use_colors)}\n"
            
            # Test Input ÏïûÏóê Ï∂îÍ∞Ä ÏòàÏ†ú ÏÇΩÏûÖ
            prompt = prompt.replace("\nTest Input:", additional_prompt + "\nTest Input:")
        
        return prompt
    
    # ============= H-ARC Í∑∏Î¶¨Îìú Ïã§Ìóò Î©îÏÑúÎìú =============
    
    def simplify_action_trace(self, actions: List[Dict]) -> List[str]:
        """Action traceÎ•º Í∞ÑÎã®Ìïú ÏÑ§Î™ÖÏúºÎ°ú Î≥ÄÌôò"""
        simplified = []
        
        for action in actions:
            action_type = action['action']
            
            if action_type == 'click_cell':
                desc = "Click on a cell"
            elif action_type == 'select_color':
                color = action.get('selected_symbol', 'unknown')
                desc = f"Select color {color}"
            elif action_type == 'flood_fill':
                desc = "Apply flood fill"
            elif action_type == 'copy':
                desc = "Copy selected area"
            elif action_type == 'paste':
                desc = "Paste copied area"
            elif action_type == 'submit':
                desc = "Submit solution"
            else:
                desc = action_type.replace('_', ' ').capitalize()
            
            simplified.append(desc)
        
        return simplified
    
    def create_harc_grid_prompt(self, task: Dict, action_traces: List[List[Dict]], 
                               num_traces: int = 3, prompt_type: str = "full_trace", 
                               use_colors: bool = False) -> str:
        """H-ARC Í∑∏Î¶¨Îìú ÏÉùÏÑ± ÌîÑÎ°¨ÌîÑÌä∏"""
        base_prompt = self.create_baseline_prompt(task, use_colors)
        
        if not action_traces or num_traces == 0:
            return base_prompt
        
        if prompt_type == "full_trace":
            # ÏÉÅÏÑ∏Ìïú action sequence ÌëúÏãú
            selected_traces = action_traces[:num_traces]
            trace_info = f"\n\nHow humans solved this task (action sequences):"
            
            for i, trace in enumerate(selected_traces):
                simplified_trace = self.simplify_action_trace(trace)
                trace_info += f"\n\nHuman Solution {i+1} ({len(trace)} steps):"
                
                for j, action in enumerate(simplified_trace[:10]):
                    trace_info += f"\n  Step {j+1}: {action}"
                
                if len(simplified_trace) > 10:
                    trace_info += f"\n  ... ({len(simplified_trace) - 10} more steps)"
        
        else:  # hint
            # ÏöîÏïΩÎêú ÌûåÌä∏Îßå ÌëúÏãú
            action_counts = defaultdict(int)
            for trace in action_traces:
                for action in trace:
                    action_counts[action['action']] += 1
            
            top_actions = sorted(action_counts.items(), key=lambda x: x[1], reverse=True)[:5]
            avg_steps = sum(len(trace) for trace in action_traces) / len(action_traces)
            
            trace_info = f"\n\nHint: Humans typically solved this using actions like: "
            trace_info += ", ".join([action[0].replace('_', ' ') for action in top_actions])
            trace_info += f"\nAverage solution length: {avg_steps:.1f} steps"
        
        # Test Input ÏïûÏóê trace Ï†ïÎ≥¥ ÏÇΩÏûÖ
        enhanced_prompt = base_prompt.replace("\nTest Input:", trace_info + "\n\nNow solve this test case:\nTest Input:")
        return enhanced_prompt
    
    # ============= H-ARC Ïï°ÏÖò Ïã§Ìóò Î©îÏÑúÎìú =============
    
    def create_action_sequence_prompt(self, task: Dict, action_traces: List[List[Dict]], 
                                     num_examples: int = 3) -> str:
        """Action sequence ÏòàÏ∏° ÌîÑÎ°¨ÌîÑÌä∏"""
        prompt = """You are analyzing how humans solve ARC tasks.
Given the training examples, predict the sequence of actions needed to solve the test case.

Available actions:
- click_cell, select_color, flood_fill, copy, paste, resize_grid, rotate, flip, submit

Training Examples:
"""
        
        for i, pair in enumerate(task.train_pairs):
            prompt += f"\nExample {i+1}:\nInput:\n{self.grid_to_string(pair.x.tolist())}\n"
            prompt += f"Output:\n{self.grid_to_string(pair.y.tolist())}\n"
        
        if action_traces and num_examples > 0:
            prompt += "\n\nHow humans solved this task (example action sequences):"
            
            for i, trace in enumerate(action_traces[:num_examples]):
                prompt += f"\n\nHuman Solution {i+1}:"
                
                for j, action in enumerate(trace):
                    action_type = action['action']
                    prompt += f"\n{j+1}. {action_type}"
                    
                    if action.get('selected_symbol') is not None:
                        prompt += f" (color: {action['selected_symbol']})"
        
        prompt += f"\n\nNow predict the action sequence for this test case:"
        prompt += f"\nTest Input:\n{self.grid_to_string(task.test_pairs[0].x.tolist())}\n"
        prompt += f"Expected Output:\n{self.grid_to_string(task.test_pairs[0].y.tolist())}\n"
        
        prompt += "\nList the actions needed to transform the input to the output."
        prompt += "\nFormat each action on a new line with a number, like:\n1. action_name\n2. action_name\n..."
        
        return prompt
    
    def parse_predicted_actions(self, response: str) -> List[str]:
        """LLM ÏùëÎãµÏóêÏÑú action sequence ÌååÏã±"""
        lines = response.strip().split('\n')
        actions = []
        
        for line in lines:
            line = line.strip()
            if re.match(r'^\d+\.', line):
                action = re.sub(r'^\d+\.\s*', '', line)
                action = re.sub(r'\s*\([^)]*\)', '', action)
                action = action.strip().lower().replace(' ', '_')
                
                normalized_action = self.normalize_action(action)
                if normalized_action:
                    actions.append(normalized_action)
        
        return actions
    
    def normalize_action(self, action: str) -> str:
        """Action Ïù¥Î¶Ñ Ï†ïÍ∑úÌôî"""
        action_lower = action.lower().strip()
        
        for standard_action, variants in self.action_mapping.items():
            if action_lower == standard_action:
                return standard_action
            for variant in variants:
                if variant in action_lower or action_lower in variant:
                    return standard_action
        
        return action
    
    def evaluate_action_sequence(self, predicted: List[str], actual_traces: List[List[Dict]]) -> Dict:
        """Action sequence ÌèâÍ∞Ä"""
        if not predicted or not actual_traces:
            return {
                'exact_match': 0, 
                'similarity': 0, 
                'common_actions_ratio': 0,
                'predicted_length': len(predicted) if predicted else 0
            }
        
        actual_sequences = []
        for trace in actual_traces:
            sequence = [self.normalize_action(action['action']) for action in trace]
            actual_sequences.append(sequence)
        
        max_similarity = 0
        best_match_info = {}
        
        for actual_seq in actual_sequences:
            if predicted == actual_seq:
                return {
                    'exact_match': 1.0,
                    'similarity': 1.0,
                    'common_actions_ratio': 1.0,
                    'predicted_length': len(predicted),
                    'actual_length': len(actual_seq),
                    'match_type': 'exact'
                }
            
            matcher = SequenceMatcher(None, predicted, actual_seq)
            similarity = matcher.ratio()
            
            common = set(predicted) & set(actual_seq)
            common_ratio = len(common) / max(len(set(predicted)), len(set(actual_seq))) if max(len(set(predicted)), len(set(actual_seq))) > 0 else 0
            
            combined_similarity = (similarity + common_ratio) / 2
            
            if combined_similarity > max_similarity:
                max_similarity = combined_similarity
                best_match_info = {
                    'exact_match': 0.0,
                    'similarity': similarity,
                    'common_actions_ratio': common_ratio,
                    'combined_similarity': combined_similarity,
                    'predicted_length': len(predicted),
                    'actual_length': len(actual_seq),
                    'match_type': 'partial'
                }
        
        return best_match_info
    
    # ============= ÌÜµÌï© Ïã§Ìóò Ïã§Ìñâ Î©îÏÑúÎìú =============
    
    def run_complete_experiment(self, task_ids: List[str] = None,
                               augmentation_sizes: List[int] = [0, 5, 10],
                               harc_trace_counts: List[int] = [0, 3],
                               harc_action_examples: List[int] = [0, 3],
                               num_candidates: int = 3) -> Dict:
        """Î™®Îì† Î™®Îç∏Î°ú Î™®Îì† Ïã§Ìóò Ïã§Ìñâ"""
        if task_ids is None:
            task_ids = ["74dd1130"]
        
        all_results = {
            'experiment_summary': {
                'models_tested': list(self.models.keys()),
                'tasks_tested': task_ids,
                'experiment_types': ['re-arc', 'h-arc-grid', 'h-arc-action']
            },
            'detailed_results': {}
        }
        
        for task_id in task_ids:
            print(f"\n{'='*80}")
            print(f"COMPLETE EXPERIMENT ON TASK: {task_id}")
            print(f"{'='*80}")
            
            # Îç∞Ïù¥ÌÑ∞ Î°úÎìú
            task = self.load_arc_task(task_id)
            if not task:
                print(f"Task {task_id} not found!")
                continue
            
            augmented_examples = self.load_rearc_augmentations(task_id)
            action_traces = self.load_harc_action_traces(task_id)
            
            task_results = {
                'task_id': task_id,
                'num_augmented_available': len(augmented_examples),
                'num_action_traces': len(action_traces),
                'model_results': {}
            }
            
            # Í∞Å Î™®Îç∏Ïóê ÎåÄÌï¥ Î™®Îì† Ïã§Ìóò ÏàòÌñâ
            for model_id, model in self.models.items():
                print(f"\n{'-'*60}")
                print(f"Testing Model: {model_id}")
                print(f"{'-'*60}")
                
                model_results = {
                    're-arc': {},
                    'h-arc-grid': {},
                    'h-arc-action': {}
                }
                
                # 1. RE-ARC Ïã§Ìóò
                print("\n1. RE-ARC Experiment")
                for num_aug in augmentation_sizes:
                    if num_aug > len(augmented_examples):
                        continue
                    
                    print(f"  Testing with {num_aug} augmented examples...")
                    
                    if num_aug == 0:
                        prompt = self.create_baseline_prompt(task, use_colors=False)
                    else:
                        prompt = self.create_augmented_prompt(task, augmented_examples, num_aug, use_colors=False)
                    
                    correct_count = 0
                    responses_and_predictions = []
                    for _ in range(num_candidates):
                        response = model.generate_response(prompt, temperature=0.3)
                        predicted_grid = self.extract_grid_from_response(response, from_colors=False)
                        expected_output = task.test_pairs[0].y.tolist()
                        
                        is_correct = self.evaluate_solution(predicted_grid, expected_output)
                        if is_correct:
                            correct_count += 1
                        
                        responses_and_predictions.append({
                            'prompt': prompt,
                            'response': response,
                            'predicted_grid': predicted_grid,
                            'expected_grid': expected_output,
                            'is_correct': is_correct
                        })
                    
                    accuracy = correct_count / num_candidates
                    model_results['re-arc'][num_aug] = {
                        'accuracy': accuracy,
                        'correct_count': correct_count,
                        'total': num_candidates,
                        'responses_and_predictions': responses_and_predictions
                    }
                    print(f"    Accuracy: {accuracy:.2%}")
                
                # 2. H-ARC Í∑∏Î¶¨Îìú Ïã§Ìóò
                print("\n2. H-ARC Grid Generation Experiment")
                for prompt_type in ['full_trace', 'hint']:
                    model_results['h-arc-grid'][prompt_type] = {}
                    
                    for num_traces in harc_trace_counts:
                        if num_traces > len(action_traces):
                            continue
                        
                        print(f"  Testing {prompt_type} with {num_traces} traces...")
                        
                        prompt = self.create_harc_grid_prompt(task, action_traces, num_traces, prompt_type, use_colors=False)
                        
                        correct_count = 0
                        responses_and_predictions = []
                        for _ in range(num_candidates):
                            response = model.generate_response(prompt, temperature=0.3)
                            predicted_grid = self.extract_grid_from_response(response, from_colors=False)
                            expected_output = task.test_pairs[0].y.tolist()
                            
                            is_correct = self.evaluate_solution(predicted_grid, expected_output)
                            if is_correct:
                                correct_count += 1
                            
                            responses_and_predictions.append({
                                'prompt': prompt,
                                'response': response,
                                'predicted_grid': predicted_grid,
                                'expected_grid': expected_output,
                                'is_correct': is_correct
                            })
                        
                        accuracy = correct_count / num_candidates
                        model_results['h-arc-grid'][prompt_type][num_traces] = {
                            'accuracy': accuracy,
                            'correct_count': correct_count,
                            'total': num_candidates,
                            'responses_and_predictions': responses_and_predictions
                        }
                        print(f"    Accuracy: {accuracy:.2%}")
                
                # 3. H-ARC Ïï°ÏÖò Ïã§Ìóò
                print("\n3. H-ARC Action Sequence Experiment")
                if action_traces:
                    for num_examples in harc_action_examples:
                        if num_examples > len(action_traces):
                            continue
                        
                        print(f"  Testing with {num_examples} action examples...")
                        
                        evaluation_results = []
                        responses_and_predictions = []
                        
                        for _ in range(num_candidates):
                            prompt = self.create_action_sequence_prompt(task, action_traces, num_examples)
                            response = model.generate_response(prompt, max_tokens=512, temperature=0.3)
                            predicted_actions = self.parse_predicted_actions(response)
                            
                            eval_result = self.evaluate_action_sequence(predicted_actions, action_traces)
                            evaluation_results.append(eval_result)
                            
                            responses_and_predictions.append({
                                'prompt': prompt,
                                'response': response,
                                'predicted_actions': predicted_actions,
                                'evaluation': eval_result
                            })
                        
                        avg_similarity = sum(r.get('combined_similarity', r.get('similarity', 0)) 
                                           for r in evaluation_results) / len(evaluation_results)
                        exact_matches = sum(r.get('exact_match', 0) for r in evaluation_results)
                        avg_common_ratio = sum(r.get('common_actions_ratio', 0) 
                                             for r in evaluation_results) / len(evaluation_results)
                        
                        model_results['h-arc-action'][num_examples] = {
                            'avg_similarity': avg_similarity,
                            'avg_common_actions_ratio': avg_common_ratio,
                            'exact_match_rate': exact_matches / num_candidates,
                            'total': num_candidates,
                            'responses_and_predictions': responses_and_predictions
                        }
                        print(f"    Avg Similarity: {avg_similarity:.3f}")
                        print(f"    Exact Match Rate: {exact_matches}/{num_candidates}")
                else:
                    print("  No action traces available for this task")
                
                task_results['model_results'][model_id] = model_results
                
                # W&B Î°úÍπÖ
                if self.use_wandb:
                    wandb.log({
                        'task_id': task_id,
                        'model_id': model_id,
                        're-arc_baseline': model_results['re-arc'].get(0, {}).get('accuracy', 0),
                        're-arc_best': max([r.get('accuracy', 0) for r in model_results['re-arc'].values()]) if model_results['re-arc'] else 0,
                        'h-arc-grid_best': max([max([r.get('accuracy', 0) for r in pt.values()]) for pt in model_results['h-arc-grid'].values()]) if model_results['h-arc-grid'] else 0,
                        'h-arc-action_best': max([r.get('avg_similarity', 0) for r in model_results['h-arc-action'].values()]) if model_results['h-arc-action'] else 0
                    })
            
            all_results['detailed_results'][task_id] = task_results
        
        # Ï¢ÖÌï© ÏöîÏïΩ ÏÉùÏÑ±
        self.generate_comprehensive_summary(all_results)
        
        return all_results
    
    def generate_comprehensive_summary(self, results: Dict):
        """Ï¢ÖÌï© ÏöîÏïΩ ÏÉùÏÑ±"""
        print("\n" + "="*80)
        print("COMPREHENSIVE EXPERIMENT SUMMARY")
        print("="*80)
        
        models = results['experiment_summary']['models_tested']
        tasks = results['experiment_summary']['tasks_tested']
        
        # Î™®Îç∏Î≥Ñ ÌèâÍ∑† ÏÑ±Îä• Í≥ÑÏÇ∞
        model_summary = {}
        
        for model_id in models:
            re_arc_scores = []
            h_arc_grid_scores = []
            h_arc_action_scores = []
            
            for task_id in tasks:
                if task_id in results['detailed_results']:
                    task_data = results['detailed_results'][task_id]
                    if model_id in task_data['model_results']:
                        model_data = task_data['model_results'][model_id]
                        
                        # RE-ARC Ï†êÏàò
                        if model_data['re-arc']:
                            re_arc_scores.extend([r.get('accuracy', 0) for r in model_data['re-arc'].values()])
                        
                        # H-ARC Grid Ï†êÏàò
                        if model_data['h-arc-grid']:
                            for prompt_type in model_data['h-arc-grid'].values():
                                h_arc_grid_scores.extend([r.get('accuracy', 0) for r in prompt_type.values()])
                        
                        # H-ARC Action Ï†êÏàò
                        if model_data['h-arc-action']:
                            h_arc_action_scores.extend([r.get('avg_similarity', 0) for r in model_data['h-arc-action'].values()])
            
            model_summary[model_id] = {
                're-arc_avg': np.mean(re_arc_scores) if re_arc_scores else 0,
                're-arc_max': max(re_arc_scores) if re_arc_scores else 0,
                'h-arc-grid_avg': np.mean(h_arc_grid_scores) if h_arc_grid_scores else 0,
                'h-arc-grid_max': max(h_arc_grid_scores) if h_arc_grid_scores else 0,
                'h-arc-action_avg': np.mean(h_arc_action_scores) if h_arc_action_scores else 0,
                'h-arc-action_max': max(h_arc_action_scores) if h_arc_action_scores else 0
            }
        
        # Í≤∞Í≥º Ï∂úÎ†•
        print(f"\n{'Model':<15} {'RE-ARC':<15} {'H-ARC Grid':<15} {'H-ARC Action':<15}")
        print(f"{'='*15} {'='*15} {'='*15} {'='*15}")
        
        for model_id, summary in model_summary.items():
            print(f"{model_id:<15} "
                  f"{summary['re-arc_max']:.1%}({summary['re-arc_avg']:.1%})<15 "
                  f"{summary['h-arc-grid_max']:.1%}({summary['h-arc-grid_avg']:.1%})<15 "
                  f"{summary['h-arc-action_max']:.3f}({summary['h-arc-action_avg']:.3f})<15")
        
        print(f"\nNote: Format is MAX(AVG) for each experiment type")
        
        # ÏµúÍ≥† ÏÑ±Îä• Î™®Îç∏ Ï∞æÍ∏∞
        best_re_arc = max(models, key=lambda m: model_summary[m]['re-arc_max'])
        best_h_arc_grid = max(models, key=lambda m: model_summary[m]['h-arc-grid_max'])
        best_h_arc_action = max(models, key=lambda m: model_summary[m]['h-arc-action_max'])
        
        print(f"\nBest Performing Models:")
        print(f"  RE-ARC: {best_re_arc} ({model_summary[best_re_arc]['re-arc_max']:.1%})")
        print(f"  H-ARC Grid: {best_h_arc_grid} ({model_summary[best_h_arc_grid]['h-arc-grid_max']:.1%})")
        print(f"  H-ARC Action: {best_h_arc_action} ({model_summary[best_h_arc_action]['h-arc-action_max']:.3f})")
        
        results['model_summary'] = model_summary
        results['best_models'] = {
            'best_re_arc': best_re_arc,
            'best_h_arc_grid': best_h_arc_grid,
            'best_h_arc_action': best_h_arc_action
        }
    
    def save_results(self, results: Dict, output_file: str = "complete_multi_model_results.json"):
        """Í≤∞Í≥º Ï†ÄÏû•"""
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"\nResults saved to: {output_file}")


def run_quick_test():
    """Îπ†Î•∏ ÌÖåÏä§Ìä∏"""
    models_config = {
        "llama3.1-8b": {
            "type": "huggingface",
            "model_name": "meta-llama/Llama-3.1-8B-Instruct"
        }
    }
    
    experiment = CompleteMultiModelExperiment(models_config, use_wandb=False)
    
    results = experiment.run_complete_experiment(
        task_ids=["74dd1130"],
        augmentation_sizes=[0, 5],
        harc_trace_counts=[0, 3],
        harc_action_examples=[0, 3],
        num_candidates=2
    )
    
    experiment.save_results(results, "quick_complete_test.json")
    print("Quick test completed!")


def get_models_config(include_openai: bool = True, include_gemma: bool = True, include_qwen: bool = True):
    """Î™®Îç∏ ÏÑ§Ï†ïÏùÑ ÏÑ†ÌÉùÏ†ÅÏúºÎ°ú ÏÉùÏÑ±"""
    models_config = {
        "llama3.1-8b": {
            "type": "huggingface",
            "model_name": "meta-llama/Llama-3.1-8B-Instruct"
        }
    }
    
    if include_openai:
        models_config["openai-o1"] = {
            "type": "openai",
            "model_name": "o1-preview",
            "api_key": ""
        }
    
    if include_gemma:
        models_config["gemma-7b"] = {
            "type": "huggingface",
            "model_name": "google/gemma-7b-it"
        }
    
    if include_qwen:
        models_config["qwen2.5-7b"] = {
            "type": "huggingface",
            "model_name": "Qwen/Qwen2.5-7B-Instruct"
        }
    
    return models_config


def print_usage():
    """ÏÇ¨Ïö©Î≤ï Ï∂úÎ†•"""
    print("""
Usage: python complete_multi_model_experiment.py [options]

Options:
  quick                 Run quick test (Llama only, 1 task, 2 candidates)
  --models MODEL1,MODEL2,...  Specify models to run (comma-separated)
                       Available: llama3.1-8b, openai-o1, gemma-7b, qwen2.5-7b
  --no-openai          Exclude OpenAI models (avoid API costs)
  --no-gemma           Exclude Gemma model
  --no-qwen            Exclude Qwen model
  --llama-only         Run with Llama 3.1-8B only
  --help               Show this help message

Examples:
  python complete_multi_model_experiment.py quick
  python complete_multi_model_experiment.py --models llama3.1-8b,gemma-7b
  python complete_multi_model_experiment.py --no-openai
  python complete_multi_model_experiment.py --llama-only
  python complete_multi_model_experiment.py --no-openai --no-gemma
""")


def main():
    """Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò"""
    import sys
    
    # Î™ÖÎ†πÌñâ Ïù∏Ïàò ÌååÏã±
    if "--help" in sys.argv or "-h" in sys.argv:
        print_usage()
        return
    
    if "quick" in sys.argv:
        run_quick_test()
        return
    
    # --models ÏòµÏÖò ÌôïÏù∏
    specific_models = None
    for i, arg in enumerate(sys.argv):
        if arg == "--models" and i + 1 < len(sys.argv):
            specific_models = sys.argv[i + 1].split(',')
            break
    
    if specific_models:
        # ÌäπÏ†ï Î™®Îç∏Îßå ÏÑ†ÌÉù
        all_models_config = get_models_config(True, True, True)
        models_config = {}
        for model_id in specific_models:
            if model_id in all_models_config:
                models_config[model_id] = all_models_config[model_id]
            else:
                print(f"Warning: Unknown model '{model_id}'. Available models: {', '.join(all_models_config.keys())}")
        
        if not models_config:
            print("Error: No valid models specified.")
            return
    else:
        # Í∏∞Ï°¥ Î°úÏßÅ: Ï†úÏô∏ ÏòµÏÖò ÏÇ¨Ïö©
        include_openai = "--no-openai" not in sys.argv
        include_gemma = "--no-gemma" not in sys.argv
        include_qwen = "--no-qwen" not in sys.argv
        
        if "--llama-only" in sys.argv:
            include_openai = False
            include_gemma = False
            include_qwen = False
        
        # Î™®Îç∏ ÏÑ§Ï†ï ÏÉùÏÑ±
        models_config = get_models_config(include_openai, include_gemma, include_qwen)
    
    # ÏÑ†ÌÉùÎêú Î™®Îç∏ Ï∂úÎ†•
    print(f"\nüöÄ Starting Complete Multi-Model Experiment")
    print(f"Selected models: {', '.join(models_config.keys())}")
    
    # OpenAI Î™®Îç∏Ïù¥ Ìè¨Ìï®ÎêòÏóàÎäîÏßÄ ÌôïÏù∏
    has_openai = any('openai' in model_id for model_id in models_config.keys())
    if not has_openai:
        print("üí∞ OpenAI models excluded (no API costs)")
    
    print(f"üìä Testing {len(models_config)} models on 4 tasks with 3 experiment types")
    estimated_time = len(models_config) * 60  # Î™®Îç∏Îãπ ÏïΩ 60Î∂Ñ
    print(f"‚è±Ô∏è  Estimated time: ~{estimated_time//60}h {estimated_time%60}m")
    print()
    
    # Ïã§Ìóò Ï¥àÍ∏∞Ìôî
    experiment = CompleteMultiModelExperiment(models_config, use_wandb=True)
    
    # ÏôÑÏ†ÑÌïú Ïã§Ìóò Ïã§Ìñâ
    results = experiment.run_complete_experiment(
        task_ids=["74dd1130"],
        augmentation_sizes=[0, 5, 10],
        harc_trace_counts=[0, 3, 5],
        harc_action_examples=[0, 3],
        num_candidates=5
    )
    
    # Í≤∞Í≥º Ï†ÄÏû•
    experiment.save_results(results)
    
    if experiment.use_wandb:
        wandb.finish()


if __name__ == "__main__":
    main()